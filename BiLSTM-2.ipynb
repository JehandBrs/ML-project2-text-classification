{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions with a BiLSTM model using Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p58OMXyeHKKE"
   },
   "outputs": [],
   "source": [
    "# import useful libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-process data to trainable input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7mXv324Khid"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "train_neg = [tweet[:-1] for tweet in open('./train_neg.txt').readlines()]\n",
    "train_pos = [tweet[:-1] for tweet in open('./train_pos.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "leNlI9IlKn4w"
   },
   "outputs": [],
   "source": [
    "# define the tokenizer and fit it on our tweets\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_neg + train_pos)\n",
    "\n",
    "# Here the representation of tweets is on a sequence.\n",
    "# Each column of the sequence correspond to an index of a word in the dictionary \n",
    "seq = tokenizer.texts_to_sequences(train_neg + train_pos)\n",
    "\n",
    "# Define y vector \n",
    "# Pad sequence X, so all tokenized tweets of X has all the same length\n",
    "X, y = pad_sequences(seq), np.array([0]*100000 + [1]*100000)\n",
    "\n",
    "# split data into train and test feature vectors\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create and train the Bi-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oJ5SXNlHn3v",
    "outputId": "ef315674-dcd6-4da9-8f36-847f4b59790a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text (InputLayer)           [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 128)         1280000   \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 200)              183200    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 30)                6030      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,469,261\n",
      "Trainable params: 1,469,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# some model parameters\n",
    "max_features=10000\n",
    "maxlen=x_train.shape[1]\n",
    "embedding_dim=128\n",
    "num_filters=200 \n",
    "\n",
    "# the model layers\n",
    "\n",
    "# embedding\n",
    "text_input=Input(shape=(None,),dtype='int32',name='text')\n",
    "embedded_text=layers.Embedding(max_features,embedding_dim,input_length=maxlen)(text_input)\n",
    "\n",
    "# Our bi-lstm layers\n",
    "# It allows for our classifier to capture context before and after each word of a text.\n",
    "x=layers.Bidirectional(layers.LSTM(100,activation='tanh',return_sequences=False, dropout=0.5, recurrent_dropout=0.1))(embedded_text)\n",
    "\n",
    "# some dropouts\n",
    "x=layers.Dropout(0.5)(x)\n",
    "\n",
    "# Neural network\n",
    "x=layers.Dense(30,activation='relu')(x)\n",
    "\n",
    "# sigmoid activation for the binary classification\n",
    "output=layers.Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "# finally assemble all this into a model\n",
    "model_biLSTM=Model(text_input,output)\n",
    "\n",
    "# let's see how it is \n",
    "model_biLSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BBK3dzqmHqNF",
    "outputId": "6bcb2e0c-6e44-49f1-8369-cc4b8207c9da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2813/2813 [==============================] - 1024s 361ms/step - loss: 0.4267 - acc: 0.7955 - val_loss: 0.3799 - val_acc: 0.8220\n",
      "Epoch 2/3\n",
      "2813/2813 [==============================] - 1007s 358ms/step - loss: 0.3634 - acc: 0.8327 - val_loss: 0.3694 - val_acc: 0.8316\n",
      "Epoch 3/3\n",
      "2813/2813 [==============================] - 1003s 357ms/step - loss: 0.3374 - acc: 0.8471 - val_loss: 0.3662 - val_acc: 0.8339\n"
     ]
    }
   ],
   "source": [
    "# Now we can train the model using a gpu\n",
    "with tf.device('/gpu:0'):\n",
    "  model_biLSTM.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "  history_biLSTM=model_biLSTM.fit(x_train,y_train,epochs=3,batch_size=64,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Submitting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fx3msTv5IRFM",
    "outputId": "adbfda89-7de7-49ef-de23-2aae39472832"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "# our validation accuracy seems pretty good\n",
    "# let's submit it on AIcrowd\n",
    "test_data = pad_sequences(tokenizer.texts_to_sequences([tweet[:-1] for tweet in open('./test_data.txt').readlines()]))\n",
    "prediction = model_biLSTM.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRecziMRPSPt"
   },
   "outputs": [],
   "source": [
    "# convert this into a csv file ready for submission\n",
    "predictions = (2*(prediction>0.5).astype(int)-1).reshape((10000,)).tolist()\n",
    "pd.DataFrame.from_dict({'Id': range(1, 10001), 'Prediction': predictions}).to_csv('submissions_dd.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
