{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.porter  import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nfrom wordcloud import WordCloud\n\nimport pandas as pd\nimport random, time\nfrom babel.dates import format_date, format_datetime, format_time\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\n\n\n# Preprocessing functions\n\n\ndef import_data(path_dataset_neg, path_dataset_pos):\n    \"\"\"\n    This function imports the data set, adds labels and returns a Pandas Dataframe, without duplicates. \n    Input : path of negative data set, path of postive dataset \n    Output: Pandas data frame with two columns : text and label\n    \"\"\"\n\n    #Kaggle version\n    train_neg = [tweet[:-1] for tweet in open(path_dataset_neg).readlines()]\n    train_pos = [tweet[:-1] for tweet in open(path_dataset_pos).readlines()]\n        \n    X, y = train_neg + train_pos, [-1 for i in range(len(train_neg))]+[1 for i in range(len(train_pos))]\n    df = pd.DataFrame(list(zip(y, X)), columns = ['label','text'], dtype = str)\n    df.drop_duplicates(inplace = True)# Delete duplicate Tweets\n    df['label'] = df['label'].astype(int)\n    \n    return df\n\n\n\n\ndef cleaning_data(df):\n    \"\"\"\n    This function removes special characters, numbers, url links, single characters  \n    Input : Pandas data frame with two columns : text and label \n    Output: Pandas data frame with two columns : text and label\n    \"\"\"\n    \n    # remove special characters from text column\n    df.text = df.text.str.replace('[#,@,&]', '')\n    \n    #Replace special characters\n    df.text = df.text.str.replace('(','')\n    df.text = df.text.str.replace(')','')\n    df.text = df.text.str.replace('=','')\n    df.text = df.text.str.replace('!','')\n    df.text = df.text.str.replace('?','')\n    df.text = df.text.str.replace('\"','')\n    df.text = df.text.str.replace('_','')\n    df.text = df.text.str.replace('-','')\n    df.text = df.text.str.replace(',','')\n    df.text = df.text.str.replace('.','')\n    df.text = df.text.str.replace(';','')\n    df.text = df.text.str.replace('+','')\n    df.text = df.text.str.replace('<user>','')\n    df.text = df.text.str.replace('<rt>','')\n    df.text = df.text.str.replace(':','')\n    df.text = df.text.str.replace('/','')\n    df.text = df.text.str.replace('<','')\n    df.text = df.text.str.replace('>','')\n    df.text = df.text.str.replace('\\'s','')\n    \n    # Remove digits\n    df.text = df.text.str.replace('\\d*','')\n    \n    #Remove www\n    df.text = df.text.str.replace('w{3}','')\n    # remove urls\n    df.text = df.text.str.replace(\"http\\S+\", \"\")\n    # remove multiple spaces with single space\n    df.text = df.text.str.replace('\\s+', ' ')\n    #remove all single characters (except \"i\")\n    df.text = df.text.str.replace(r'\\s+[a-hA-H]\\s+', '')\n    df.text = df.text.str.replace(r'\\s+[j-zJ-Z]\\s+', '')\n    df.text = df.text.str.replace(r'\\s+[i-iI-I]\\s+',' ')\n    return df\n\n\n\ndef remove_stopwords(df):\n    \n    \"\"\"\n    This function stopwords, defined in the list in the function.\n    We delete Twitter specific words, english stopwords, but we keep negative forms of verbs and negative adverbs\n    Input : Pandas data frame with two columns : text and label \n    Output: Pandas data frame with two columns : text and label\n    \"\"\"\n    \n    stop_words = ['i', 'me', 'my', 'myself', 'we','url' 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain']\n    stop_words.extend(['u', 'wa', 'ha','ho', 'would', 'com', 'user','<user>', '<rt>' 'url', 'rt', 'custom picture', 'i\\'m', 'picture frame','<url>', 'positer frame', 'x','i\\'ll'])\n    stop_words.remove('not')\n    stop_words.remove('no')\n    stop_words.remove('nor')\n    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n    return df\n\n\ndef Porter_stemmer(df):\n    \"\"\"\n    This function applies Porter Stemmer methodology to reduces words to their stem\n    Input : Pandas data frame with two columns : text and label \n    Output: Pandas data frame with two columns : text and label\n    \"\"\"   \n    stemmer = PorterStemmer()\n    df['text'] = df['text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n    return df\n\ndef snow_ball_stemmer(df):\n    \"\"\"\n    This function applies Snowball Stemmer methodology to reduces words to their stem\n    Input : Pandas data frame with two columns : text and label \n    Output: Pandas data frame with two columns : text and label\n    \"\"\"   \n    snow_stemmer = SnowballStemmer(language='english')\n    df['text'] = df['text'].apply(lambda x: ' '.join([snow_stemmer.stem(word) for word in x.split()]))\n    return df\n\ndef lemmatize_text(df):\n    \"\"\"\n    This function applies World Net Lemmatizing methodology to reduces words to their stem\n    Input : Pandas data frame with two columns : text and label \n    Output: Pandas data frame with two columns : text and label\n    \"\"\"   \n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    df['text'] = df['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n    return df\n\n\ndef Basic_Vectorizer(df):\n    \"\"\"\n    This function transforms text into a matrix mapping X using all words in text as vocabulary list \n    It also transform the labels to a numpy vector y\n    Input : Pandas data frame with two columns : text and label \n    Output:  X vector of features, y vector of labels\n    \"\"\"   \n    text = df['text']\n    y = df['label'].to_numpy()\n    \n    basic_vectorizer = CountVectorizer(binary=True)\n    basic_vectorizer.fit(text)\n    X = basic_vectorizer.transform(text)\n    \n    return X, y\n\n\n\ndef N_Gram_Vectorizer(df, N):\n    \"\"\"\n    This function transforms text into a matrix mapping X using all words in text as vocabulary list.\n    It maps N-grams (series of N consecutive words)\n    It also transform the labels to a numpy vector y\n    Input : Pandas data frame with two columns : text and label, N the parameter for N-grams \n    Output:  X vector of features, y vector of labels\n    \"\"\"   \n    text = df['text']\n    y = df['label'].to_numpy()\n    \n    #adding two or three word sequences (bigrams or trigrams)\n    ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, N))\n    ngram_vectorizer.fit(text)\n    X = ngram_vectorizer.transform(text)\n    \n    return X, y\n\n\ndef SVD_preprocessing(X, y, N):\n    \n    \"\"\"\n    This function applies SVD transformation to the features matrix X, keeping the N most significant drivers\n    Input : Matrix of features X, vector of labels y, parameter N for number of drivers to keep\n    Output:  X vector of features after SVD, y vector of labels\n    \"\"\"  \n    clf = TruncatedSVD(100)\n    X_SVD = clf.fit_transform(X)\n    \n    return X_SVD, y","metadata":{"_uuid":"37afffdb-4674-4077-88d4-ae679c8b1c87","_cell_guid":"fb4e2fc3-a887-4468-97aa-1b95b5336ab4","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}