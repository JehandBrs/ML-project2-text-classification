{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.porter  import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nfrom wordcloud import WordCloud\n\nimport pandas as pd\nimport random, time\nfrom babel.dates import format_date, format_datetime, format_time\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD","metadata":{"_uuid":"fd5d8500-853a-4847-a181-f963c569afd4","_cell_guid":"f4e4073f-18d0-456f-8da9-81a06d0c66c3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-22T12:23:31.366133Z","iopub.execute_input":"2022-12-22T12:23:31.366717Z","iopub.status.idle":"2022-12-22T12:23:31.378018Z","shell.execute_reply.started":"2022-12-22T12:23:31.366662Z","shell.execute_reply":"2022-12-22T12:23:31.376678Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-12-22T12:23:31.380306Z","iopub.execute_input":"2022-12-22T12:23:31.381172Z","iopub.status.idle":"2022-12-22T12:23:31.400049Z","shell.execute_reply.started":"2022-12-22T12:23:31.381124Z","shell.execute_reply":"2022-12-22T12:23:31.398534Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"/kaggle/input/tweetsdataset/train_neg.txt\n/kaggle/input/tweetsdataset/train_pos.txt\n/kaggle/input/tweetsdataset/test_data.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"path_dataset_neg = \"/kaggle/input/tweetsdataset/train_neg.txt\"\npath_dataset_pos = \"/kaggle/input/tweetsdataset/train_pos.txt\"","metadata":{"execution":{"iopub.status.busy":"2022-12-22T12:23:31.401920Z","iopub.execute_input":"2022-12-22T12:23:31.402570Z","iopub.status.idle":"2022-12-22T12:23:31.409124Z","shell.execute_reply.started":"2022-12-22T12:23:31.402533Z","shell.execute_reply":"2022-12-22T12:23:31.407732Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nimport pandas","metadata":{"execution":{"iopub.status.busy":"2022-12-22T12:23:31.412404Z","iopub.execute_input":"2022-12-22T12:23:31.413672Z","iopub.status.idle":"2022-12-22T12:23:31.431302Z","shell.execute_reply.started":"2022-12-22T12:23:31.413623Z","shell.execute_reply":"2022-12-22T12:23:31.429989Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing functions","metadata":{"_uuid":"af097f6f-b537-4abe-a336-f3b12c144e9c","_cell_guid":"002db445-7753-4fdc-8355-ebd96942ee11","trusted":true}},{"cell_type":"code","source":"def import_data(path_dataset_neg, path_dataset_pos):\n    \"\"\"\n    This function imports the data set, adds labels and returns a Pandas Dataframe, without duplicates. \n    Input : path of negative data set, path of postive dataset \n    Output: Pandas data frame with two columns : text and label\n    \"\"\"\n\n    #Kaggle version\n    train_neg = [tweet[:-1] for tweet in open(path_dataset_neg).readlines()]\n    train_pos = [tweet[:-1] for tweet in open(path_dataset_pos).readlines()]\n        \n    X, y = train_neg + train_pos, [-1 for i in range(len(train_neg))]+[1 for i in range(len(train_pos))]\n    df = pd.DataFrame(list(zip(y, X)), columns = ['label','text'], dtype = str)\n    df.drop_duplicates(inplace = True)# Delete duplicate Tweets\n    df['label'] = df['label'].astype(int)\n    \n    return df","metadata":{"_uuid":"141d6022-39cf-4374-a469-fb47ba8ad31e","_cell_guid":"fac5abfc-d7ec-41f5-b15d-4b7069dad347","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-22T12:23:31.433017Z","iopub.execute_input":"2022-12-22T12:23:31.433756Z","iopub.status.idle":"2022-12-22T12:23:31.444298Z","shell.execute_reply.started":"2022-12-22T12:23:31.433711Z","shell.execute_reply":"2022-12-22T12:23:31.443405Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def cleaning_data(df):\n    \"\"\"\n    This function removes special characters, numbers, url links, single characters  \n    Input : Pandas data frame with two columns : text and label \n    Output: Pandas data frame with two columns : text and label\n    \"\"\"\n    \n    # remove special characters from text column\n    df.text = df.text.str.replace('[#,@,&]', '')\n    \n    #Replace special characters\n    df.text = df.text.str.replace('(','')\n    df.text = df.text.str.replace(')','')\n    df.text = df.text.str.replace('=','')\n    df.text = df.text.str.replace('!','')\n    df.text = df.text.str.replace('?','')\n    df.text = df.text.str.replace('\"','')\n    df.text = df.text.str.replace('_','')\n    df.text = df.text.str.replace('-','')\n    df.text = df.text.str.replace(',','')\n    df.text = df.text.str.replace('.','')\n    df.text = df.text.str.replace(';','')\n    df.text = df.text.str.replace('+','')\n    df.text = df.text.str.replace('<user>','')\n    df.text = df.text.str.replace('<rt>','')\n    df.text = df.text.str.replace(':','')\n    df.text = df.text.str.replace('/','')\n    df.text = df.text.str.replace('<','')\n    df.text = df.text.str.replace('>','')\n    df.text = df.text.str.replace('\\'s','')\n    \n    # Remove digits\n    df.text = df.text.str.replace('\\d*','')\n    \n    #Remove www\n    df.text = df.text.str.replace('w{3}','')\n    # remove urls\n    df.text = df.text.str.replace(\"http\\S+\", \"\")\n    # remove multiple spaces with single space\n    df.text = df.text.str.replace('\\s+', ' ')\n    #remove all single characters (except \"i\")\n    df.text = df.text.str.replace(r'\\s+[a-hA-H]\\s+', '')\n    df.text = df.text.str.replace(r'\\s+[j-zJ-Z]\\s+', '')\n    df.text = df.text.str.replace(r'\\s+[i-iI-I]\\s+',' ')\n    return df","metadata":{"_uuid":"66416daf-9118-4e16-8db6-583c09702026","_cell_guid":"1a65b60b-b0fd-42cb-9276-37505784d15e","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-22T12:23:31.445747Z","iopub.execute_input":"2022-12-22T12:23:31.446897Z","iopub.status.idle":"2022-12-22T12:23:31.461658Z","shell.execute_reply.started":"2022-12-22T12:23:31.446861Z","shell.execute_reply":"2022-12-22T12:23:31.460669Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def remove_stopwords(df):\n    \n    \"\"\"\n    This function stopwords, defined in the list in the function.\n    We delete Twitter specific words, english stopwords, but we keep negative forms of verbs and negative adverbs\n    Input : Pandas data frame with two columns : text and label \n    Output: Pandas data frame with two columns : text and label\n    \"\"\"\n    \n    stop_words = ['i', 'me', 'my', 'myself', 'we','url' 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain']\n    stop_words.extend(['u', 'wa', 'ha','ho', 'would', 'com', 'user','<user>', '<rt>' 'url', 'rt', 'custom picture', 'i\\'m', 'picture frame','<url>', 'positer frame', 'x','i\\'ll'])\n    stop_words.remove('not')\n    stop_words.remove('no')\n    stop_words.remove('nor')\n    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n    return df","metadata":{"_uuid":"621fff25-2566-4543-a3d2-3d829b9e24ce","_cell_guid":"c362ba8c-af51-41e5-b99e-cfeb67ec8e9b","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-22T12:23:31.463319Z","iopub.execute_input":"2022-12-22T12:23:31.464073Z","iopub.status.idle":"2022-12-22T12:23:31.480906Z","shell.execute_reply.started":"2022-12-22T12:23:31.464030Z","shell.execute_reply":"2022-12-22T12:23:31.479298Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def Porter_stemmer(df):\n    \"\"\"\n    This function applies Porter Stemmer methodology to reduces words to their stem\n    Input : Pandas data frame with two columns : text and label \n    Output: Pandas data frame with two columns : text and label\n    \"\"\"   \n    stemmer = PorterStemmer()\n    df['text'] = df['text'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n    return df\n\ndef snow_ball_stemmer(df):\n    \"\"\"\n    This function applies Snowball Stemmer methodology to reduces words to their stem\n    Input : Pandas data frame with two columns : text and label \n    Output: Pandas data frame with two columns : text and label\n    \"\"\"   \n    snow_stemmer = SnowballStemmer(language='english')\n    df['text'] = df['text'].apply(lambda x: ' '.join([snow_stemmer.stem(word) for word in x.split()]))\n    return df\n\ndef lemmatize_text(df):\n    \"\"\"\n    This function applies World Net Lemmatizing methodology to reduces words to their stem\n    Input : Pandas data frame with two columns : text and label \n    Output: Pandas data frame with two columns : text and label\n    \"\"\"   \n    lemmatizer = nltk.stem.WordNetLemmatizer()\n    df['text'] = df['text'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n    return df","metadata":{"_uuid":"e81647bc-21c9-4e3c-b3a5-4e152f426cc9","_cell_guid":"7dbcaa49-40aa-486d-b1a7-d03c6ae64186","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-22T12:23:31.483011Z","iopub.execute_input":"2022-12-22T12:23:31.483564Z","iopub.status.idle":"2022-12-22T12:23:31.497640Z","shell.execute_reply.started":"2022-12-22T12:23:31.483507Z","shell.execute_reply":"2022-12-22T12:23:31.496507Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Vectorizer","metadata":{"_uuid":"fe5a1f53-467e-4787-9686-f4665b5695ec","_cell_guid":"a4edb48d-2db2-4b28-ae61-b8e17c3cf442","trusted":true}},{"cell_type":"code","source":"def Basic_Vectorizer(df):\n    \"\"\"\n    This function transforms text into a matrix mapping X using all words in text as vocabulary list \n    It also transform the labels to a numpy vector y\n    Input : Pandas data frame with two columns : text and label \n    Output:  X vector of features, y vector of labels\n    \"\"\"   \n    text = df['text']\n    y = df['label'].to_numpy()\n    \n    basic_vectorizer = CountVectorizer(binary=True)\n    basic_vectorizer.fit(text)\n    X = basic_vectorizer.transform(text)\n    \n    return X, y\n\n\n\ndef N_Gram_Vectorizer(df, N):\n    \"\"\"\n    This function transforms text into a matrix mapping X using all words in text as vocabulary list.\n    It maps N-grams (series of N consecutive words)\n    It also transform the labels to a numpy vector y\n    Input : Pandas data frame with two columns : text and label, N the parameter for N-grams \n    Output:  X vector of features, y vector of labels\n    \"\"\"   \n    text = df['text']\n    y = df['label'].to_numpy()\n    \n    #adding two or three word sequences (bigrams or trigrams)\n    ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, N))\n    ngram_vectorizer.fit(text)\n    X = ngram_vectorizer.transform(text)\n    \n    return X, y","metadata":{"_uuid":"f181e0e3-feb4-464e-ae49-6cc604d152be","_cell_guid":"e230458f-ac6e-4c91-b71a-3727987a6f47","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-22T12:23:31.499062Z","iopub.execute_input":"2022-12-22T12:23:31.499595Z","iopub.status.idle":"2022-12-22T12:23:31.516310Z","shell.execute_reply.started":"2022-12-22T12:23:31.499550Z","shell.execute_reply":"2022-12-22T12:23:31.515133Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# SVD","metadata":{"_uuid":"442d7a68-50fb-419e-99c8-5d946172e227","_cell_guid":"fbdf376b-b18d-40f1-b726-41aa75296161","trusted":true}},{"cell_type":"code","source":"def SVD_preprocessing(X, y, N):\n    \n    \"\"\"\n    This function applies SVD transformation to the features matrix X, keeping the N most significant drivers\n    Input : Matrix of features X, vector of labels y, parameter N for number of drivers to keep\n    Output:  X vector of features after SVD, y vector of labels\n    \"\"\"  \n    clf = TruncatedSVD(100)\n    X_SVD = clf.fit_transform(X)\n    \n    return X_SVD, y","metadata":{"_uuid":"8c721af8-0123-4a2f-9cf1-d43499887a55","_cell_guid":"24fb011f-ec94-4bd5-b453-0f954e5197f5","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-22T12:23:31.520128Z","iopub.execute_input":"2022-12-22T12:23:31.520725Z","iopub.status.idle":"2022-12-22T12:23:31.535187Z","shell.execute_reply.started":"2022-12-22T12:23:31.520527Z","shell.execute_reply":"2022-12-22T12:23:31.533990Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing methods for testing","metadata":{"_uuid":"a891f3aa-2a27-4fc2-8228-ff7c98e5382c","_cell_guid":"76716510-0bce-487a-95b2-cc9d9bbf1fa0","trusted":true}},{"cell_type":"code","source":"def evaluate_method(X,y, message):\n    \"\"\"\n    This function first splits the data X, y into training and testing sets\n    It then trains and test with a Logistic regression model and display the score\n    Input : Matrix of features X, vector of labels y, message to display\n    Output:  Nonen, but prints accuracy of model\n    \"\"\"   \n    random.seed(42)\n    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.75)\n    lr = LogisticRegression()\n    lr.fit(X_train, y_train)\n    print (\"Accuracy \"+message+\" : %s\" % (accuracy_score(y_val, lr.predict(X_val))))\n    return None","metadata":{"_uuid":"ba1589c4-8355-40c9-b376-3779c2ced7d8","_cell_guid":"98514962-a99f-41f7-897c-51535f234d99","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-22T12:23:31.536397Z","iopub.execute_input":"2022-12-22T12:23:31.537001Z","iopub.status.idle":"2022-12-22T12:23:31.546545Z","shell.execute_reply.started":"2022-12-22T12:23:31.536967Z","shell.execute_reply":"2022-12-22T12:23:31.545273Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def test_clean_data(path_dataset_neg, path_dataset_pos):\n    \n    \"\"\"\n    This function compares the accuracy of a logistic model, with and without a given processing method : cleaning data\n    Input : path of negative data set, path of postive dataset\n    Output:  Nonen, but prints accuracy of different processing method\n    \"\"\"   \n    \n    #Prediction accuracy with clean data\n    df1 = import_data(path_dataset_neg, path_dataset_pos)\n    df1 = cleaning_data(df1)\n    X1, y1 = Basic_Vectorizer(df1)\n    evaluate_method(X1, y1, \"with clean data\")\n    \n    #Prediction accuracy without clean data\n    \n    df2 = import_data(path_dataset_neg, path_dataset_pos)\n    X2, y2 = Basic_Vectorizer(df2)\n    evaluate_method(X2, y2, \"without clean data\")\n    \n    #Solution of test\n    \n    #Accuracy with clean data : 0.7924378460656063\n    #Accuracy without clean data : 0.7940041031523681\n    #As the Accuracy is equal, we keep this process\n    \n    return None\n\ntest_clean_data(path_dataset_neg, path_dataset_pos)","metadata":{"_uuid":"9d19e9e8-0b18-4817-b410-5e0f517a097c","_cell_guid":"c26ebdd0-9010-4c1a-89b9-08c8246e3d1d","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-22T12:23:31.548271Z","iopub.execute_input":"2022-12-22T12:23:31.549223Z","iopub.status.idle":"2022-12-22T12:24:16.844884Z","shell.execute_reply.started":"2022-12-22T12:23:31.549187Z","shell.execute_reply":"2022-12-22T12:24:16.843578Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"},{"name":"stdout","text":"Accuracy with clean data : 0.7884008735743752\nAccuracy without clean data : 0.7922172464759215\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"}]},{"cell_type":"code","source":"def test_stopwords(path_dataset_neg, path_dataset_pos):\n    \"\"\"\n    This function compares the accuracy of a logistic model, with and without a given processing method : stopwords\n    We keep the methods : cleaning data,  that are relevant according to the precedant test\n    Input : path of negative data set, path of postive dataset\n    Output:  None, but prints accuracy of different processing method\n    \"\"\"   \n    \n    #Prediction accuracy with  deleting stopwords\n    df1 = import_data(path_dataset_neg, path_dataset_pos)\n    df1 = cleaning_data(df1)\n    df1 = remove_stopwords(df1)\n    X1, y1 = Basic_Vectorizer(df1)\n    evaluate_method(X1, y1, \"with deleting stopwords\")\n    \n    #Prediction accuracy without deleting stopwords\n    \n    df2 = import_data(path_dataset_neg, path_dataset_pos)\n    df2 = cleaning_data(df2)\n    X2, y2 = Basic_Vectorizer(df2)\n    evaluate_method(X2, y2, \"without deleting stopwords\")\n    \n    #Solution of test\n    \n    #Accuracy with deleting stopwords : 0.7828858838322561\n    #Accuracy without deleting stopwords : 0.790011250579074\n    #As the Accuracy is equal, we should not keep this process\n    \n    \n    return None\n\ntest_stopwords(path_dataset_neg, path_dataset_pos)","metadata":{"_uuid":"3402ef2f-1642-4aae-95b4-b8c3beaae3ad","_cell_guid":"5791271d-07db-4ecd-b677-d9c92e3643c4","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-22T12:24:16.847041Z","iopub.execute_input":"2022-12-22T12:24:16.855847Z","iopub.status.idle":"2022-12-22T12:25:15.150599Z","shell.execute_reply.started":"2022-12-22T12:24:16.855762Z","shell.execute_reply":"2022-12-22T12:25:15.148972Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"},{"name":"stdout","text":"Accuracy with deleting stopwords : 0.7857757384571264\nAccuracy without deleting stopwords : 0.7922172464759215\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"}]},{"cell_type":"code","source":"def test_normalization(path_dataset_neg, path_dataset_pos):\n    \"\"\"\n    This function compares the accuracy of a logistic model, with and without a given processing method : normalization methods : Porter Stemming, Snowball stemming, Lemmatizing\n    We keep the methods : cleaning data, remove stopwords, that are relevant according to the precedant test\n    Input : path of negative data set, path of postive dataset\n    Output:  None, but prints accuracy of different processing method\n    \"\"\"  \n    \n    #Prediction accuracy without  deleting normalization\n    df1 = import_data(path_dataset_neg, path_dataset_pos)\n    df1 = cleaning_data(df1)\n    df1 = remove_stopwords(df1)#voir si on garde\n    X1, y1 = Basic_Vectorizer(df1)\n    evaluate_method(X1, y1, \"without normalization\")\n    \n    #Prediction accuracy with Porter\n    df2 = import_data(path_dataset_neg, path_dataset_pos)\n    df2 = cleaning_data(df2)\n    df2 = remove_stopwords(df2) #voir si on garde\n    df2 = Porter_stemmer(df2)\n    X2, y2 = Basic_Vectorizer(df2)\n    evaluate_method(X2, y2, \"with Porter Stemmer\")\n    \n    \n    #Prediction accuracy with SnowBall Stemmer\n    df3 = import_data(path_dataset_neg, path_dataset_pos)\n    df3 = cleaning_data(df3)\n    df3 = remove_stopwords(df3) #voir si on garde\n    df3 = snow_ball_stemmer(df3)\n    X3, y3 = Basic_Vectorizer(df3)\n    evaluate_method(X3, y3, \"with Snowball Stemmer\")   \n    \n    \n    #Prediction accuracy with Lemmatizer\n    df4 = import_data(path_dataset_neg, path_dataset_pos)\n    df4 = cleaning_data(df4)\n    df4 = remove_stopwords(df4) #voir si on garde\n    #df4 = lemmatize_text(df4)\n    \n    X4, y4 = Basic_Vectorizer(df4)\n    evaluate_method(X4, y4, \"with lemmatization\")   \n    \n    \n    #Solution of test\n    \n    #Accuracy without normalization : 0.782091725309391\n    #Accuracy with Porter Stemmer : 0.7836359224371843\n    #Accuracy with Snowball Stemmer : 0.7831285433809093\n    #Accuracy with lemmatization : XXX\n    #As the Accuracy is nearly equal, we keep method Porter Stemmer, that is the best for reproductibility of results and is short to implement\n    \n    return None\n\ntest_normalization(path_dataset_neg, path_dataset_pos)","metadata":{"_uuid":"cad5c92b-59aa-47f7-bf8a-5def62982966","_cell_guid":"bc36fbd8-5bff-4043-be98-5011edb8c5dd","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-22T12:25:15.153608Z","iopub.execute_input":"2022-12-22T12:25:15.154948Z","iopub.status.idle":"2022-12-22T12:28:06.003456Z","shell.execute_reply.started":"2022-12-22T12:25:15.154872Z","shell.execute_reply":"2022-12-22T12:28:06.002248Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"},{"name":"stdout","text":"Accuracy without normalization : 0.7856654386622841\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"},{"name":"stdout","text":"Accuracy with Porter Stemmer : 0.7826432242836028\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"},{"name":"stdout","text":"Accuracy with Snowball Stemmer : 0.781076967196841\nAccuracy with lemmatization : 0.7837462222320266\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"}]},{"cell_type":"code","source":"def test_vectorization(path_dataset_neg, path_dataset_pos):\n    \n    \"\"\"\n    This function compares the accuracy of a logistic model, with and without a given vectorization method : 2-Grams, 3-Grams.\n    We keep the methods : cleaning data, remove stopwords and Porter Stemmer, that are relevant according to the precedant test\n    Input : path of negative data set, path of postive dataset\n    Output:  None, but prints accuracy of different processing method\n    \"\"\"  \n    \n    #Prediction accuracy with basic vectorization\n    df1 = import_data(path_dataset_neg, path_dataset_pos)\n    df1 = cleaning_data(df1)\n    df1 = remove_stopwords(df1)#voir si on garde\n    df1 = Porter_stemmer(df1)\n    X1, y1 = Basic_Vectorizer(df1)\n    evaluate_method(X1, y1, \"with basic vectorization\")\n    \n    \n    #Prediction accuracy with 2-Grams vectorization\n    df1 = import_data(path_dataset_neg, path_dataset_pos)\n    df1 = cleaning_data(df1)\n    df1 = remove_stopwords(df1)#voir si on garde\n    df1 = Porter_stemmer(df1)\n    X1, y1 = N_Gram_Vectorizer(df1,2)\n    evaluate_method(X1, y1, \"with 2-grams vectorization\")\n    \n    \n    \n    #Prediction accuracy with 3-Grams vectorization\n    df1 = import_data(path_dataset_neg, path_dataset_pos)\n    df1 = cleaning_data(df1)\n    df1 = remove_stopwords(df1)#voir si on garde\n    df1 = Porter_stemmer(df1)\n    X1, y1 = N_Gram_Vectorizer(df1,3)\n    evaluate_method(X1, y1, \"with 3-grams vectorization\")\n    \n    \n    #Solution of test\n    \n    #Accuracy with basic vectorization : 0.7799739692484172\n    #Accuracy with 2-grams vectorization : 0.7945776620855485\n    #Accuracy with 3-grams vectorization : 0.7926143257373541\n    \n\n    return None\n\ntest_vectorization(path_dataset_neg, path_dataset_pos)","metadata":{"_uuid":"826b4fa0-50c3-42fa-8b7e-e14b43e2bf5f","_cell_guid":"d2ffe7ae-4ebf-45b0-8c65-98003f687084","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-22T12:28:06.004993Z","iopub.execute_input":"2022-12-22T12:28:06.005651Z","iopub.status.idle":"2022-12-22T12:32:39.409823Z","shell.execute_reply.started":"2022-12-22T12:28:06.005614Z","shell.execute_reply":"2022-12-22T12:32:39.408436Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"},{"name":"stdout","text":"Accuracy with basic vectorization : 0.7809887273609671\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"},{"name":"stdout","text":"Accuracy with 2-grams vectorization : 0.7919304670093313\nAccuracy with 3-grams vectorization : 0.792283426352827\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n","output_type":"stream"}]},{"cell_type":"code","source":"def test_SVD(path_dataset_neg, path_dataset_pos, N):\n    \"\"\"\n    This function compares the accuracy of a logistic model, with and without a given processing method : SVD \n    We keep the methods : cleaning data, remove stopwords and Porter Stemmer, 2-Grams Vectorizer that are relevant according to the precedant test\n    Input : path of negative data set, path of postive dataset, Dimension of residual training set \n    Output:  Nonen, but prints accuracy of different processing method\n    \"\"\"  \n    \n    #Prediction accuracy with basic vectorization\n    df1 = import_data(path_dataset_neg, path_dataset_pos)\n    df1 = cleaning_data(df1)\n    df1 = remove_stopwords(df1)#voir si on garde\n    df1 = Porter_stemmer(df1)\n    X1, y1 = N_Gram_Vectorizer(df1,2)\n    X1, y1 = SVD_preprocessing(X1, y1, N)\n    message = \"with PCA \" +str(N)\n    evaluate_method(X1, y1, message)\n    return None\n\n#Accuracy with PCA 10 000 : 0.7068010853499812\n\ntest_SVD(path_dataset_neg, path_dataset_pos, 10000)","metadata":{"_uuid":"59622ee0-1ed7-47d9-9d4f-cb7253d5c0b1","_cell_guid":"ced68c99-0501-4209-a93b-f84793199644","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-12-22T12:34:40.603522Z","iopub.execute_input":"2022-12-22T12:34:40.604012Z","iopub.status.idle":"2022-12-22T12:36:50.041677Z","shell.execute_reply.started":"2022-12-22T12:34:40.603977Z","shell.execute_reply":"2022-12-22T12:36:50.039976Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Accuracy with PCA 10000 : 0.7016611149103262\n","output_type":"stream"}]}]}