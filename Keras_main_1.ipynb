{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3opjbSq1xrcFzz56XiOuE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheGreatJanus/ML-project2-text-classification/blob/main/Keras_main_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 411,
      "metadata": {
        "id": "DjlIpFPJj1_1"
      },
      "outputs": [],
      "source": [
        "#From Tenserflow\n",
        "from keras import Sequential\n",
        "from keras.layers import Embedding, Dense, GlobalAveragePooling1D, Dropout\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "\n",
        "import pandas as pd\n",
        "import gzip\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import warnings; warnings.simplefilter('ignore')\n",
        "import os, codecs, string, random\n",
        "import numpy as np\n",
        "from numpy.random import seed as random_seed\n",
        "from numpy.random import shuffle as random_shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "#NLP libraries\n",
        "import spacy, nltk, gensim, sklearn\n",
        "\n",
        "\n",
        "#Scikit imports\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vd5im1Ass20",
        "outputId": "9425afc6-a4b3-416f-df74-caf0585b4d3b"
      },
      "execution_count": 412,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import small train and test datasets\n",
        "train_neg = [tweet[:-1] for tweet in open('/twitter-datasets/train_neg.txt').readlines()]\n",
        "train_pos = [tweet[:-1] for tweet in open('/twitter-datasets/train_pos.txt').readlines()]\n",
        "test_data = [tweet[:-1] for tweet in open('/twitter-datasets/test_data.txt').readlines()]"
      ],
      "metadata": {
        "id": "ztsoGfATkuZQ"
      },
      "execution_count": 413,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = train_neg + train_pos, [0 for i in range(len(train_neg))]+[1 for i in range(len(train_pos))]\n",
        "X_test, y_test = test_data, [0 for i in range(len(test_data))] \n",
        "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.1)"
      ],
      "metadata": {
        "id": "dMX_ZmvNnjrS"
      },
      "execution_count": 414,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_test = np.array(X_test).reshape(-1)\n",
        "#X_train = np.array(X_train).reshape(-1)\n",
        "#X_eval = np.array(X_eval).reshape(-1)\n",
        "\n",
        "#y_test = np.array(y_test).reshape(-1)\n",
        "#y_train = np.array(y_train).reshape(-1)\n",
        "#y_eval = np.array(y_eval).reshape(-1)\n"
      ],
      "metadata": {
        "id": "R1iO0ZIG5VZw"
      },
      "execution_count": 415,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "raw_eval_ds = tf.data.Dataset.from_tensor_slices((X_eval, y_eval))\n",
        "raw_test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
      ],
      "metadata": {
        "id": "4xRLhBpAmDR_"
      },
      "execution_count": 416,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shuffle and Batch\n",
        "BATCH_SIZE = 64\n",
        "SHUFFLE_BUFFER_SIZE = 100\n",
        "\n",
        "#train_dataset = raw_train_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "#eval_dataset = raw_eval_ds.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "MWzhcmjxktxb"
      },
      "execution_count": 417,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 94953\n",
        "sequence_length = 100\n",
        "\n",
        "vectorize_layer = layers.TextVectorization(max_tokens=max_features,\n",
        "    standardize= 'lower_and_strip_punctuation',\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length\n",
        ")"
      ],
      "metadata": {
        "id": "WECzxQ2jma0c"
      },
      "execution_count": 418,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a text-only dataset (without labels), then call adapt\n",
        "train_text = raw_train_ds.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(train_text)"
      ],
      "metadata": {
        "id": "Wwu834aAnEo5"
      },
      "execution_count": 419,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_text(text, label):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return vectorize_layer(text), label"
      ],
      "metadata": {
        "id": "LP-pjmOnmqYF"
      },
      "execution_count": 420,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for element in raw_train_ds:\n",
        "  print(element)\n",
        "  i+=1\n",
        "  if i==10 :break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOw7eM6zqhu7",
        "outputId": "d6d67a58-5791-427a-e5f4-3c3af5ecb12c"
      },
      "execution_count": 421,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'babe i have plans to go to the mall'>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b\"zatarain's new orleans style white beans & rice , 7 - ounce boxes ( pack of 12 this easy-to-prepare dinner mix has ... <url>\">, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'dont wanna hear that its really hot in portugal'>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b\"i've been going to bed early lately , i like the extra sleep\">, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'the great ribs book ( paperback the authors of our popular hot series ( over 200,000 books in print ) team up to ... <url>'>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b\"i have so much track dirt on my jeans from #fenway100 ... i think i shall retire these pants and frame ' em\">, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'<user> on the upside , its wednesday .. half way to the weekend for you lol'>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'<user> <user> he smells shooo good <3 damn b smelling a nigga and everything :p'>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'<user> aha follow it'>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'thanks for the mention ! jesssturman <url> much appreciated ! #jenny_meszaros'>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" 3134 ---> \",vectorize_layer.get_vocabulary()[1500])\n",
        "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShCTJqAwm5IM",
        "outputId": "8c1ba9c1-1b9c-486f-ed62-82c1121ab353"
      },
      "execution_count": 422,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 3134 --->  peace\n",
            "Vocabulary size: 94953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = raw_train_ds.map(vectorize_text)\n",
        "val_ds = raw_eval_ds.map(vectorize_text)\n",
        "test_ds = raw_test_ds.map(vectorize_text)"
      ],
      "metadata": {
        "id": "IelumtiznAJT"
      },
      "execution_count": 423,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "MDjUPgntseH4"
      },
      "execution_count": 424,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating the model**"
      ],
      "metadata": {
        "id": "FZnfFzBdsfda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 4"
      ],
      "metadata": {
        "id": "eW7aYlWtsh6_"
      },
      "execution_count": 425,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  layers.Embedding(max_features + 1, embedding_dim),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(1),\n",
        "  layers.Flatten()])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7Go_H07skCP",
        "outputId": "1141d1bf-a143-425a-a8f6-edba89b16bf5"
      },
      "execution_count": 426,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_24 (Embedding)    (None, None, 4)           379816    \n",
            "                                                                 \n",
            " dropout_48 (Dropout)        (None, None, 4)           0         \n",
            "                                                                 \n",
            " global_average_pooling1d_24  (None, 4)                0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dropout_49 (Dropout)        (None, 4)                 0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 379,821\n",
            "Trainable params: 379,821\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
      ],
      "metadata": {
        "id": "rrNgVd8lvex7"
      },
      "execution_count": 427,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "vzudNvD-vf4Z",
        "outputId": "9a4407fe-bb3d-488d-9989-73b2617fe1a3"
      },
      "execution_count": 428,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-428-271bfd77b202>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     epochs=epochs)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 139, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 243, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1930, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5283, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 1) vs ()).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)"
      ],
      "metadata": {
        "id": "cVQJP3QdxPLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "metadata": {
        "id": "k0S2Z9cqxUD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history_dict['binary_accuracy']\n",
        "val_acc = history_dict['val_binary_accuracy']\n",
        "loss = history_dict['loss']\n",
        "val_loss = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ei_wLjtnxW1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mLVOwjhmxYh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_model = tf.keras.Sequential([\n",
        "  vectorize_layer,\n",
        "  model,\n",
        "  layers.Activation('sigmoid')\n",
        "])\n",
        "\n",
        "export_model.compile(\n",
        "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Test it with `raw_test_ds`, which yields raw strings\n",
        "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "Vpt7H4PFxjEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_model.predict(examples)"
      ],
      "metadata": {
        "id": "-uJQ8_1uxmwT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}