{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7203ea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import os, codecs, string, random\n",
    "import numpy as np\n",
    "from numpy.random import seed as random_seed\n",
    "from numpy.random import shuffle as random_shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "#NLP libraries\n",
    "import spacy, nltk, gensim, sklearn\n",
    "#import pyLDAvis.gensim\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "#Vader\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "#Scikit imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df655913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import small train and test datasets\n",
    "train_neg = [tweet[:-1] for tweet in open('./twitter-datasets/train_neg.txt').readlines()]\n",
    "train_pos = [tweet[:-1] for tweet in open('./twitter-datasets/train_pos.txt').readlines()]\n",
    "test_data = [tweet[:-1] for tweet in open('./twitter-datasets/test_data.txt').readlines()]\n",
    "\n",
    "# import vocab_cut\n",
    "vocab = [word[:-1] for word in open('./vocab_cut.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "864fd80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 101298)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(vocabulary = vocab)\n",
    "X_train_counts = count_vect.fit_transform(train_neg)\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)\n",
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ff83ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1a61263",
   "metadata": {},
   "source": [
    "## Pipeline using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7cd170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipl = Pipeline([\n",
    "    ('vect', CountVectorizer(vocabulary = vocab, ngram_range = (1, 5))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('logreg', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ea738af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.81385\n"
     ]
    }
   ],
   "source": [
    "X, y = train_neg + train_pos, [-1 for i in range(len(train_neg))]+[1 for i in range(len(train_pos))]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "pipl.fit(X_train, y_train)\n",
    "prediction = pipl.predict(X_test)\n",
    "print('accuracy = {}'.format(accuracy_score(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabb07d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187776c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a submission to AIcrowd\n",
    "df = pd.DataFrame.from_dict({'Id' : range(1, 10001), 'Prediction': prediction.tolist()})\n",
    "df.to_csv('submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd91bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f3937e3",
   "metadata": {},
   "source": [
    "## pipeline using decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "262a8590",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipl = Pipeline([\n",
    "    ('vect', CountVectorizer(vocabulary = vocab, ngram_range = (1, 2))),\n",
    "    ('clf', DecisionTreeClassifier(max_depth=50)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eabb8eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7577\n"
     ]
    }
   ],
   "source": [
    "X, y = train_neg + train_pos, [-1 for i in range(len(train_neg))]+[1 for i in range(len(train_pos))]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "pipl.fit(X_train, y_train)\n",
    "prediction = pipl.predict(X_test)\n",
    "print('accuracy = {}'.format(accuracy_score(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747845e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55cbefa8",
   "metadata": {},
   "source": [
    "## Pipeline using neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc5fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a630d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipl = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range = (1, 1))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('mlp', MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 2), random_state=1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc733486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont work\n",
    "X, y = train_neg + train_pos, [-1 for i in range(len(train_neg))]+[1 for i in range(len(train_pos))]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pipl.fit(X_train, y_train)\n",
    "prediction = pipl.predict(X_test)\n",
    "accuracy_score(y_test, prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
